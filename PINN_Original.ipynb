{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "from compute_distance import compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "h_n = 50\n",
    "input_n = 4\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net3(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            ################## below are added layers\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = r\"models\\original\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "e_idx = -1\n",
    "model_1_path = os.path.join(weights_dir, f\"original_1_epoch_{e_idx}.pth\")\n",
    "model_2_path = os.path.join(weights_dir, f\"original_2_epoch_{e_idx}.pth\")\n",
    "model_3_path = os.path.join(weights_dir, f\"original_3_epoch_{e_idx}.pth\")\n",
    "\n",
    "def initialize_models():\n",
    "    # Move models to GPU\n",
    "    model_1 = Net1().to(device)\n",
    "    model_2 = Net2().to(device)\n",
    "    model_3 = Net3().to(device)\n",
    "\n",
    "    print(f\"Model_1 is on: {next(model_1.parameters()).device}\")\n",
    "    print(f\"Model_2 is on: {next(model_2.parameters()).device}\")\n",
    "    print(f\"Model_3 is on: {next(model_3.parameters()).device}\")\n",
    "\n",
    "\n",
    "    optimizer_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "    optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "    optimizer_3 = optim.Adam(model_3.parameters(), lr=1e-3)\n",
    "\n",
    "        # Load saved weights if they exist\n",
    "    if os.path.exists(model_1_path):\n",
    "        model_1.load_state_dict(torch.load(model_1_path))\n",
    "        model_1.train()\n",
    "        print(f\"✅ Loaded weights from {model_1_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(model_2_path):\n",
    "        model_2.load_state_dict(torch.load(model_2_path))\n",
    "        model_2.train()\n",
    "        print(f\"✅ Loaded weights from {model_2_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(model_3_path):\n",
    "        model_3.load_state_dict(torch.load(model_3_path))\n",
    "        model_3.train()\n",
    "        print(f\"✅ Loaded weights from {model_3_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_3_path}. Training from scratch.\")\n",
    "\n",
    "    return model_1, model_2, model_3, optimizer_1, optimizer_2, optimizer_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss, v_loss_weight, p_loss_weight, rho=1.0, nu=0.01):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - model_1, model_2, model_3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    # Compute model predictions inside criterion\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    y.requires_grad_(True)\n",
    "    d.requires_grad_(True)\n",
    "    n.requires_grad_(True)\n",
    "\n",
    "    model_inputs = torch.cat((x, y, d, n), dim=1)  # Concatenate inputs for the models\n",
    "\n",
    "    # some sort of ways to actually define the pairs inside of these 50000 points.\n",
    "    u = model_1(model_inputs)\n",
    "    v = model_2(model_inputs)\n",
    "    p = model_3(model_inputs)\n",
    "    #print(u.shape)\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "    #print(u.shape)\n",
    "    \n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "\n",
    "    boundary_condition = (distances < 1e-6)  # Small tolerance\n",
    "    u = torch.where(boundary_condition, torch.zeros_like(u), u)\n",
    "    v = torch.where(boundary_condition, torch.zeros_like(v), v)\n",
    "\n",
    "    inlet_condition = (torch.abs(x) < 1e-6)\n",
    "    v = torch.where(inlet_condition, torch.zeros_like(v), v)\n",
    "\n",
    "    u_loss = mse_loss(u, y_batch[:, 0:1])\n",
    "    v_loss = mse_loss(v, y_batch[:, 1:2])\n",
    "    p_loss = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "\n",
    "    # Compute data loss (MSE against target values)\n",
    "    L_data = ( u_loss + v_loss * v_loss_weight + p_loss / p_loss_weight )\n",
    "    #L_data_periodic = ( u_p_loss + v_loss * 500 + p_loss )\n",
    "\n",
    "    # l_data_periodic + L\n",
    "\n",
    "\n",
    "    return L_data, u_loss, v_loss, p_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c29478-f7a8-40cf-974f-cbb8f04ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss track\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "\n",
    "    csv_dir      = os.path.join(os.getcwd(), \"data\", \"unscaled\")\n",
    "    pattern      = os.path.join(csv_dir, \"W_*_*_1.csv\") \n",
    "    csv_paths    = sorted(glob.glob(pattern))           \n",
    "\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "\n",
    "    frames = [pd.read_csv(p) for p in csv_paths]\n",
    "    df     = pd.concat(frames, ignore_index=True)\n",
    "    data        = df.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # file_path = os.path.join(os.getcwd(), \"data/unscaled/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # # Load data using pandas and convert to NumPy array\n",
    "    # df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    # data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=100000, shuffle=True)\n",
    "    print(drt)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    model_1, model_2, model_3, optimizer_1, optimizer_2, optimizer_3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 500\n",
    "    mse_loss = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # Lists to store loss history\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'total_loss': [],\n",
    "        'u_loss': [],\n",
    "        'v_loss': [],\n",
    "        'p_loss': []\n",
    "    }\n",
    "\n",
    "    v_loss_weight = 50\n",
    "    p_loss_weight = 1\n",
    "    \n",
    "    for epoch in range(e_idx+1, num_epochs+1):\n",
    "        total_loss, total_u_loss, total_v_loss, total_p_loss = 0, 0, 0, 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        if epoch >= 100:\n",
    "            v_loss_weight = 100\n",
    "            # p_loss_weight = 10\n",
    "\n",
    "        if epoch >= 200:\n",
    "            v_loss_weight = 200\n",
    "            # p_loss_weight = 20\n",
    "        \n",
    "        if epoch >= 300:\n",
    "            v_loss_weight = 300\n",
    "            # p_loss_weight = 50\n",
    "\n",
    "        if epoch >= 400:\n",
    "            v_loss_weight = 500\n",
    "            # p_loss_weight = 100\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, u_loss, v_loss, p_loss = criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss, v_loss_weight, p_loss_weight)\n",
    "\n",
    "    \n",
    "            optimizer_1.zero_grad()\n",
    "            optimizer_2.zero_grad()\n",
    "            optimizer_3.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_1.step()\n",
    "            optimizer_2.step()\n",
    "            optimizer_3.step()\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_u_loss += u_loss.item()\n",
    "            total_v_loss += v_loss.item()\n",
    "            total_p_loss += p_loss.item()\n",
    "    \n",
    "        # Compute average loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_u_loss = total_u_loss / num_batches\n",
    "        avg_v_loss = total_v_loss / num_batches\n",
    "        avg_p_loss = total_p_loss / num_batches\n",
    "        \n",
    "        # Store losses in history\n",
    "        loss_history['epoch'].append(epoch)\n",
    "        loss_history['total_loss'].append(avg_loss)\n",
    "        loss_history['u_loss'].append(avg_u_loss)\n",
    "        loss_history['v_loss'].append(avg_v_loss)\n",
    "        loss_history['p_loss'].append(avg_p_loss)\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(model_1.state_dict(), f'models/original/original_1_epoch_{epoch}.pth')\n",
    "            torch.save(model_2.state_dict(), f'models/original/original_2_epoch_{epoch}.pth')\n",
    "            torch.save(model_3.state_dict(), f'models/original/original_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "\n",
    "        elif epoch % 100 == 0:\n",
    "            min_loss = avg_loss\n",
    "            # Save models every 100 epochs\n",
    "            torch.save(model_1.state_dict(), f'models/original/original_1_epoch_{epoch}.pth')\n",
    "            torch.save(model_2.state_dict(), f'models/original/original_2_epoch_{epoch}.pth')\n",
    "            torch.save(model_3.state_dict(), f'models/original/original_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: Models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg ULoss: {avg_u_loss:.7f}, \"\n",
    "                  f\"Avg VLoss: {avg_v_loss:.7f}, Avg PLoss: {avg_p_loss:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "    loss_df.to_csv('results/original/training_losses_original.csv', index=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(loss_history['epoch'], loss_history['total_loss'], label='Total Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['u_loss'], label='U Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['v_loss'], label='V Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['p_loss'], label='P Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/original/loss_curves_original.png')\n",
    "    plt.show()\n",
    "\n",
    "    return model_1, model_2, model_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1, model_2, model_3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfda07-be96-4a24-96f5-88d9af7bcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560778-32d8-4a85-8a47-ba46ff6e96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION USING HARD ENFORCEMENT\n",
    "\n",
    "# File path\n",
    "file_path = \"data/unscaled/W_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d, N)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (u, v, p)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define function to compute the minimum distance from the nearest circular post\n",
    "def compute_distance(x, y, d, n):\n",
    "    \"\"\"Compute the minimum distance of (x, y) from the nearest circular post.\"\"\"\n",
    "    tilt = 0.4 / n\n",
    "    centers = [(0, 0), (0, 0.4), (0.4, tilt), (0.4, 0.4 + tilt)]\n",
    "    \n",
    "    distances = torch.full_like(x, float(\"inf\"))  # Initialize distances with large values\n",
    "\n",
    "    for cx, cy in centers:\n",
    "        r = d / 2\n",
    "        distance_to_post = torch.sqrt((x - cx) ** 2 + (y - cy) ** 2) - r\n",
    "        distances = torch.minimum(distances, distance_to_post)  # Take the minimum distance\n",
    "\n",
    "    return torch.maximum(distances, torch.tensor(0.0, device=device))  # Set negative distances to zero\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Extract individual elements\n",
    "x, y, d, n = input_tensor[:, 0], input_tensor[:, 1], input_tensor[:, 2], input_tensor[:, 3]\n",
    "\n",
    "# Apply conditions\n",
    "v_pred[x == 0] = 0  # If x = 0, set v_pred to zero\n",
    "\n",
    "# Compute distance\n",
    "distances = compute_distance(x, y, d, n)\n",
    "\n",
    "# If distance is zero, set u and v to zero\n",
    "mask = distances == 0\n",
    "u_pred[mask] = 0\n",
    "v_pred[mask] = 0\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab819-01e5-43e9-b954-4f54c38c145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for NO MODIFICATION\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"data/unscaled/W_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input (same format as training)\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (x, y, d)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131cda-ad99-4ec3-9f9b-930076f7abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = input_data[:, 0].reshape(-1)\n",
    "y_grid = input_data[:, 1].reshape(-1)\n",
    "v_actual_grid = output_data[:, 1].reshape(-1)\n",
    "v_pred_grid = v_pred.flatten()\n",
    "u_actual_grid = output_data[:, 0].reshape(-1)\n",
    "u_pred_grid = u_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9beeee-c7e0-4bc5-95f2-b8c1f08b41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x_grid,\n",
    "    'y': y_grid,\n",
    "    'u': u_pred_grid,\n",
    "    'v': v_pred_grid\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('results/original/csv/O_0.55_14.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83f0c-41dc-43c0-a508-01e19fa44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ed29b-ed83-4cae-9024-8ef28858d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14fe0d-bef8-44ca-95ca-099e46f48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ece96-a3df-4962-bbac-34c537436f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
