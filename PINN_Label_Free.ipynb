{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "from compute_distance import compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679a7688-0958-4875-b603-ac0afd60f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "h_nD = 30\n",
    "h_n = 20\n",
    "input_n = 4\n",
    "learning_rate = 5e-4\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net3(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            ################## below are added layers\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da46a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = r\"models\\label_free\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "e_idx = -1\n",
    "num_epochs = 500\n",
    "net1_path = os.path.join(weights_dir, f\"label_free_1_epoch_{e_idx}.pth\")\n",
    "net2_path = os.path.join(weights_dir, f\"label_free_2_epoch_{e_idx}.pth\")\n",
    "net3_path = os.path.join(weights_dir, f\"label_free_3_epoch_{e_idx}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    # Move models to GPU\n",
    "    net1 = Net1().to(device)\n",
    "    net2 = Net2().to(device)\n",
    "    net3 = Net3().to(device)\n",
    "\n",
    "    print(f\"net1 is on: {next(net1.parameters()).device}\")\n",
    "    print(f\"net2 is on: {next(net2.parameters()).device}\")\n",
    "    print(f\"net3 is on: {next(net3.parameters()).device}\")\n",
    "\n",
    "    def init_normal(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # use the modules apply function to recursively apply the initialization   \n",
    "    net1.apply(init_normal)\n",
    "    net2.apply(init_normal)\n",
    "    net3.apply(init_normal)\n",
    "\n",
    "\n",
    "    optimizer1 = optim.Adam(net1.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer2\t= optim.Adam(net2.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer3\t= optim.Adam(net3.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\n",
    "    if os.path.exists(net1_path):\n",
    "        net1.load_state_dict(torch.load(net1_path))\n",
    "        net1.train()\n",
    "        print(f\"✅ Loaded weights from {net1_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net2_path):\n",
    "        net2.load_state_dict(torch.load(net2_path))\n",
    "        net2.train()\n",
    "        print(f\"✅ Loaded weights from {net2_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net3_path):\n",
    "        net3.load_state_dict(torch.load(net3_path))\n",
    "        net3.train()\n",
    "        print(f\"✅ Loaded weights from {net3_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net3_path}. Training from scratch.\")\n",
    "\n",
    "    return net1, net2, net3, optimizer1, optimizer2, optimizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7885fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models_lbfgs():\n",
    "    # Move models to GPU\n",
    "    net1 = Net1().to(device)\n",
    "    net2 = Net2().to(device)\n",
    "    net3 = Net3().to(device)\n",
    "\n",
    "    print(f\"net1 is on: {next(net1.parameters()).device}\")\n",
    "    print(f\"net2 is on: {next(net2.parameters()).device}\")\n",
    "    print(f\"net3 is on: {next(net3.parameters()).device}\")\n",
    "\n",
    "    def init_normal(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # Initialize weights\n",
    "    net1.apply(init_normal)\n",
    "    net2.apply(init_normal)\n",
    "    net3.apply(init_normal)\n",
    "\n",
    "    # Load saved weights (if any)\n",
    "    if os.path.exists(net1_path):\n",
    "        net1.load_state_dict(torch.load(net1_path))\n",
    "        net1.train()\n",
    "        print(f\"✅ Loaded weights from {net1_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net2_path):\n",
    "        net2.load_state_dict(torch.load(net2_path))\n",
    "        net2.train()\n",
    "        print(f\"✅ Loaded weights from {net2_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net3_path):\n",
    "        net3.load_state_dict(torch.load(net3_path))\n",
    "        net3.train()\n",
    "        print(f\"✅ Loaded weights from {net3_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net3_path}. Training from scratch.\")\n",
    "\n",
    "    # ✅ Combine all parameters into one LBFGS optimizer\n",
    "    optimizer = optim.LBFGS(\n",
    "        list(net1.parameters()) + list(net2.parameters()) + list(net3.parameters()),\n",
    "        lr=1.0,\n",
    "        max_iter=20,\n",
    "        history_size=50,\n",
    "        tolerance_grad=1e-7,\n",
    "        line_search_fn='strong_wolfe'\n",
    "    )\n",
    "\n",
    "    return net1, net2, net3, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37fd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_relative_mse(pred, target, scale=1.0, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes mean squared relative error, scaled for balance across loss terms.\n",
    "    \"\"\"\n",
    "    relative_error = (pred - target) / (target.abs() + eps)\n",
    "    return torch.mean(relative_error ** 2) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x_batch, y_batch, net1, net2, net3, mse_loss, rho=1.0, nu=0.01, ds = 0.4):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - net1, net2, net3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "\n",
    "    x.requires_grad_()\n",
    "    y.requires_grad_()\n",
    "    d.requires_grad_()\n",
    "    n.requires_grad_()\n",
    "\n",
    "    # net_in = torch.cat((x, y, d), dim=1)\n",
    "    net_in = torch.cat((x, y, d, n), dim=1)\n",
    "\n",
    "    u = net1(net_in)\n",
    "    v = net2(net_in)\n",
    "    p = net3(net_in)\n",
    "\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "\n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "\n",
    "    u_hard = u * distances\n",
    "    v_hard = distances * v \n",
    "    p_hard = p\n",
    "\n",
    "    u_avg = 0.01 / d\n",
    "    u_max = (3 / 2) * u_avg\n",
    "\n",
    "    # Instead of in-place modifications:\n",
    "    boundary_condition = (distances < 1e-6)  # Small tolerance\n",
    "    u_hard = torch.where(boundary_condition, torch.zeros_like(u_hard), u_hard)\n",
    "    v_hard = torch.where(boundary_condition, torch.zeros_like(v_hard), v_hard)\n",
    "\n",
    "    inlet_condition = (torch.abs(x) < 1e-6)\n",
    "    u_inlet = u_max * (1 - (4*(((ds / 2) - y) ** 2)) / ((ds - d) ** 2))\n",
    "    u_hard = torch.where(inlet_condition, u_inlet, u_hard)\n",
    "    v_hard = torch.where(inlet_condition, torch.zeros_like(v), v_hard)\n",
    "\n",
    "    u_x = torch.autograd.grad(u_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_y = torch.autograd.grad(u_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    p_x = torch.autograd.grad(p_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    #P_xx = torch.autograd.grad(p_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    loss_1 = (u_hard*u_x+v_hard*u_y-nu*(u_xx+u_yy)+1/rho*p_x)\n",
    "\n",
    "    v_x = torch.autograd.grad(v_hard,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_y = torch.autograd.grad(v_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    P_y = torch.autograd.grad(p_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    #P_yy = torch.autograd.grad(P_y,y,grad_outputs=torch.ones_like(x),create_graph = True,allow_unused = True)[0]\n",
    "\n",
    "\n",
    "    loss_2 = (u_hard*v_x+v_hard*v_y - nu*(v_xx+v_yy)+1/rho*P_y)\n",
    "    #Main_deriv = torch.cat((u_x,u_xx,u_y,u_yy,P_x,v_x,v_xx,v_y,v_yy,P_y),1)\n",
    "    loss_3 = (u_x + v_y)\n",
    "    #loss_3 = u_x**2 + 2*u_y*v_x + v_y**2+1/rho*(P_xx + P_yy)\n",
    "    #loss_3 = loss_3*100\n",
    "\n",
    "    loss_1 = mse_loss(loss_1, torch.zeros_like(loss_1))\n",
    "    loss_2 = mse_loss(loss_2, torch.zeros_like(loss_2))\n",
    "    loss_3 = mse_loss(loss_3, torch.zeros_like(loss_3))\n",
    "\n",
    "    loss_4 = mse_loss(u, y_batch[:, 0:1])\n",
    "    loss_5 = mse_loss(v, y_batch[:, 1:2])\n",
    "    loss_6 = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "    loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_6\n",
    "\n",
    "    return loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74347fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_lbfgs(x_batch, y_batch, net1, net2, net3, mse_loss, rho=1.0, nu=0.01, ds = 0.4):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - net1, net2, net3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "\n",
    "    x.requires_grad_()\n",
    "    y.requires_grad_()\n",
    "    d.requires_grad_()\n",
    "    n.requires_grad_()\n",
    "\n",
    "    net_in = torch.cat((x, y, d, n), dim=1)\n",
    "\n",
    "    u = net1(net_in)\n",
    "    v = net2(net_in)\n",
    "    p = net3(net_in)\n",
    "\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "\n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "\n",
    "    u_hard = u\n",
    "    v_hard = v\n",
    "    p_hard = p\n",
    "\n",
    "    u_avg = 0.01 / d\n",
    "    u_max = (3 / 2) * u_avg\n",
    "\n",
    "    # Instead of in-place modifications:\n",
    "    boundary_condition = (distances < 1e-6)  # Small tolerance\n",
    "    u_hard = torch.where(boundary_condition, torch.zeros_like(u_hard), u_hard)\n",
    "    v_hard = torch.where(boundary_condition, torch.zeros_like(v_hard), v_hard)\n",
    "\n",
    "    inlet_condition = (torch.abs(x) < 1e-6)\n",
    "    u_inlet = u_max * (1 - (4*(((ds / 2) - y) ** 2)) / ((ds - d) ** 2))\n",
    "    u_hard = torch.where(inlet_condition, u_inlet, u_hard)\n",
    "    v_hard = torch.where(inlet_condition, torch.zeros_like(v), v_hard)\n",
    "\n",
    "    u_x = torch.autograd.grad(u_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_y = torch.autograd.grad(u_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    p_x = torch.autograd.grad(p_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    #P_xx = torch.autograd.grad(p_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    loss_1 = (u_hard*u_x+v_hard*u_y-nu*(u_xx+u_yy)+1/rho*p_x)\n",
    "\n",
    "    v_x = torch.autograd.grad(v_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_y = torch.autograd.grad(v_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    p_y = torch.autograd.grad(p_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    #P_yy = torch.autograd.grad(P_y,y,grad_outputs=torch.ones_like(x),create_graph = True,allow_unused = True)[0]\n",
    "\n",
    "\n",
    "    loss_2 = (u_hard*v_x+v_hard*v_y - nu*(v_xx+v_yy)+1/rho*p_y)\n",
    "    #Main_deriv = torch.cat((u_x,u_xx,u_y,u_yy,P_x,v_x,v_xx,v_y,v_yy,P_y),1)\n",
    "    loss_3 = (u_x + v_y)\n",
    "    #loss_3 = u_x**2 + 2*u_y*v_x + v_y**2+1/rho*(P_xx + P_yy)\n",
    "    #loss_3 = loss_3*100\n",
    "\n",
    "    loss_1 = mse_loss(loss_1, torch.zeros_like(loss_1))\n",
    "    loss_2 = mse_loss(loss_2, torch.zeros_like(loss_2))\n",
    "    loss_3 = mse_loss(loss_3, torch.zeros_like(loss_3))\n",
    "\n",
    "    loss_4 = mse_loss(u, y_batch[:, 0:1])\n",
    "    loss_5 = mse_loss(v, y_batch[:, 1:2])\n",
    "    loss_6 = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "    loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_6\n",
    "\n",
    "    return loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c29478-f7a8-40cf-974f-cbb8f04ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss track\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "    # file_path = os.path.join(os.getcwd(), \"data/unscaled/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # Load data using pandas and convert to NumPy array\n",
    "    # df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    # data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "\n",
    "    csv_dir      = os.path.join(os.getcwd(), \"data\", \"unscaled\")\n",
    "    pattern      = os.path.join(csv_dir, \"W_*_*_1.csv\") \n",
    "    csv_paths    = sorted(glob.glob(pattern))           \n",
    "\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "\n",
    "    frames = [pd.read_csv(p) for p in csv_paths]\n",
    "    df     = pd.concat(frames, ignore_index=True)\n",
    "    data        = df.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=100000, shuffle=True)\n",
    "    print(drt)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    net1, net2, net3, optimizer1, optimizer2, optimizer3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    mse_loss = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # Lists to store loss history\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'batch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': [],\n",
    "        'loss_6': []\n",
    "    }\n",
    "\n",
    "    loss_history_by_epoch = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': [],\n",
    "        'loss_6': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(e_idx+1, num_epochs):\n",
    "        total_loss, total_loss_1, total_loss_2, total_loss_3, total_loss_4, total_loss_5, total_loss_6 = 0, 0, 0, 0, 0, 0, 0\n",
    "        num_batches = len(dataloader)\n",
    "        batch_idx = 0\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "            net1.zero_grad()\n",
    "            net2.zero_grad()\n",
    "            net3.zero_grad()\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6 = criterion(x_batch, y_batch, net1, net2, net3, mse_loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "\n",
    "            # Store losses in history\n",
    "            loss_history['epoch'].append(epoch)\n",
    "            loss_history['batch'].append(batch_idx)\n",
    "            loss_history['loss'].append(loss.item())\n",
    "            loss_history['loss_1'].append(loss_1.item())\n",
    "            loss_history['loss_2'].append(loss_2.item())\n",
    "            loss_history['loss_3'].append(loss_3.item())\n",
    "            loss_history['loss_4'].append(loss_4.item())\n",
    "            loss_history['loss_5'].append(loss_5.item())\n",
    "            loss_history['loss_6'].append(loss_6.item())\n",
    "\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_loss_1 += loss_1.item()\n",
    "            total_loss_2 += loss_2.item()\n",
    "            total_loss_3 += loss_3.item()\n",
    "            total_loss_4 += loss_4.item()\n",
    "            total_loss_5 += loss_5.item()\n",
    "            total_loss_6 += loss_6.item()\n",
    "\n",
    "            batch_idx += 1\n",
    "    \n",
    "        # Compute average loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_loss_1 = total_loss_1 / num_batches\n",
    "        avg_loss_2 = total_loss_2 / num_batches\n",
    "        avg_loss_3 = total_loss_3 / num_batches\n",
    "        avg_loss_4 = total_loss_4 / num_batches\n",
    "        avg_loss_5 = total_loss_5 / num_batches\n",
    "        avg_loss_6 = total_loss_6 / num_batches\n",
    "\n",
    "        # Store average losses by epoch\n",
    "        loss_history_by_epoch['epoch'].append(epoch)\n",
    "        loss_history_by_epoch['loss'].append(avg_loss)\n",
    "        loss_history_by_epoch['loss_1'].append(avg_loss_1)\n",
    "        loss_history_by_epoch['loss_2'].append(avg_loss_2)\n",
    "        loss_history_by_epoch['loss_3'].append(avg_loss_3)\n",
    "        loss_history_by_epoch['loss_4'].append(avg_loss_4)\n",
    "        loss_history_by_epoch['loss_5'].append(avg_loss_5)\n",
    "        loss_history_by_epoch['loss_6'].append(avg_loss_6)\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "\n",
    "        elif epoch % 100 == 0:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: Models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg Loss_1: {avg_loss_1:.7f}, \"\n",
    "                  f\"Avg Loss_2: {avg_loss_2:.7f}, Avg Loss_3: {avg_loss_3:.7f}, Avg Loss_4: {avg_loss_4:.7f}, Avg Loss_5: {avg_loss_5:.7f}, Avg Loss_6: {avg_loss_6:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_500.pth')\n",
    "    torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_500.pth')\n",
    "    torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_500.pth')\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "\n",
    "    loss_csv_path = 'results/label_free/training_losses_label_free.csv'\n",
    "    \n",
    "    if os.path.exists(loss_csv_path):\n",
    "        existing_loss_df = pd.read_csv(loss_csv_path)\n",
    "    else:\n",
    "        existing_loss_df = None\n",
    "\n",
    "    if existing_loss_df is not None:\n",
    "        loss_df = pd.concat([existing_loss_df, loss_df], ignore_index=True)\n",
    "\n",
    "    loss_df.to_csv('results/label_free/training_losses_label_free.csv', index=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss'], label='Total Loss')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_1'], label='Loss 1')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_2'], label='Loss 2')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_3'], label='Loss 3')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_4'], label='Loss 4')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_5'], label='Loss 5')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_6'], label='Loss 6')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/label_free/loss_curves_label_free.png')\n",
    "    plt.show()\n",
    "\n",
    "    return net1, net2, net3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e02126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_lbfgs():\n",
    "    from torch.nn import MSELoss\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load all training data (full-batch)\n",
    "    csv_dir   = os.path.join(os.getcwd(), \"data\", \"unscaled\")\n",
    "    pattern   = os.path.join(csv_dir, \"W_*_*_1.csv\")\n",
    "    csv_paths = sorted(glob.glob(pattern))\n",
    "\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "\n",
    "    frames = [pd.read_csv(p) for p in csv_paths]\n",
    "    df     = pd.concat(frames, ignore_index=True)\n",
    "    data   = df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    dataset = CustomDataset(data)\n",
    "    x_batch, y_batch = dataset[:]  # full batch\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "    # Load models\n",
    "    net1, net2, net3, optimizer = initialize_models_lbfgs()\n",
    "    mse_loss = MSELoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    min_loss = float('inf')\n",
    "\n",
    "    # Logging storage\n",
    "    loss_history = {k: [] for k in ['epoch', 'loss', 'loss_1', 'loss_2', 'loss_3', 'loss_4', 'loss_5', 'loss_6']}\n",
    "\n",
    "    latest_losses = {}\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6  = criterion_lbfgs(x_batch, y_batch, net1, net2, net3, mse_loss)\n",
    "        loss.backward()\n",
    "\n",
    "        latest_losses['loss'] = loss\n",
    "        latest_losses['loss_1'] = loss_1\n",
    "        latest_losses['loss_2'] = loss_2\n",
    "        latest_losses['loss_3'] = loss_3\n",
    "        latest_losses['loss_4'] = loss_4\n",
    "        latest_losses['loss_5'] = loss_5\n",
    "        latest_losses['loss_6'] = loss_6\n",
    "\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # Get detailed loss breakdown for logging\n",
    "        loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6 = criterion_lbfgs(\n",
    "            x_batch, y_batch, net1, net2, net3, mse_loss)\n",
    "\n",
    "        # Log\n",
    "        loss_history['epoch'].append(epoch)\n",
    "        loss_history['loss'].append(latest_losses['loss'].item())\n",
    "        loss_history['loss_1'].append(latest_losses['loss_1'].item())\n",
    "        loss_history['loss_2'].append(latest_losses['loss_2'].item())\n",
    "        loss_history['loss_3'].append(latest_losses['loss_3'].item())\n",
    "        loss_history['loss_4'].append(latest_losses['loss_4'].item())\n",
    "        loss_history['loss_5'].append(latest_losses['loss_5'].item())\n",
    "        loss_history['loss_6'].append(latest_losses['loss_6'].item())\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[{epoch}/{num_epochs}] Total Loss: {loss.item():.2f} | PDE: Loss 1: {loss_1.item():.2f}, Loss 2: {loss_2.item():.2f}, Loss 3: {loss_3.item():.2f} | Loss 4: {loss_4.item():.2f}, Loss 5: {loss_5.item():.2f}, Loss 6: {loss_6.item():.2f} | Time: {elapsed_time:.2f}s\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if loss.item() < min_loss:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "        elif epoch % 100 == 0:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: Models saved\")\n",
    "\n",
    "    torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_500.pth')\n",
    "    torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_500.pth')\n",
    "    torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_500.pth')\n",
    "            \n",
    "    # Save loss log\n",
    "    os.makedirs(\"results/label_free\", exist_ok=True)\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "    loss_df.to_csv(\"results/label_free/losses_lbfgs.csv\", index=False)\n",
    "\n",
    "    # Save loss plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for k in loss_history:\n",
    "        if k != 'epoch':\n",
    "            plt.plot(loss_history['epoch'], loss_history[k], label=k)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"LBFGS Losses (Log Scale)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/label_free/loss_curve_lbfgs.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return net1, net2, net3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1 is on: cuda:0\n",
      "net2 is on: cuda:0\n",
      "net3 is on: cuda:0\n",
      "⚠️ No saved weights found at models\\label_free\\label_free_1_epoch_-1.pth. Training from scratch.\n",
      "⚠️ No saved weights found at models\\label_free\\label_free_2_epoch_-1.pth. Training from scratch.\n",
      "⚠️ No saved weights found at models\\label_free\\label_free_3_epoch_-1.pth. Training from scratch.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 25.78 GiB is allocated by PyTorch, and 300.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m net1, net2, net3 = \u001b[43mmain_lbfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mmain_lbfgs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Get detailed loss breakdown for logging\u001b[39;00m\n\u001b[32m     52\u001b[39m     loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6 = criterion_lbfgs(\n\u001b[32m     53\u001b[39m         x_batch, y_batch, net1, net2, net3, mse_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lbfgs.py:330\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    327\u001b[39m state.setdefault(\u001b[33m\"\u001b[39m\u001b[33mn_iter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m orig_loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m loss = \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[32m    332\u001b[39m current_evals = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mmain_lbfgs.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m     34\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6  = \u001b[43mcriterion_lbfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     loss.backward()\n\u001b[32m     38\u001b[39m     latest_losses[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m] = loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mcriterion_lbfgs\u001b[39m\u001b[34m(x_batch, y_batch, net1, net2, net3, mse_loss, rho, nu, ds)\u001b[39m\n\u001b[32m     62\u001b[39m v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(x),create_graph = \u001b[38;5;28;01mTrue\u001b[39;00m,only_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     64\u001b[39m v_y = torch.autograd.grad(v_hard,y,grad_outputs=torch.ones_like(y),create_graph = \u001b[38;5;28;01mTrue\u001b[39;00m,only_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m v_yy = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     67\u001b[39m p_y = torch.autograd.grad(p_hard,y,grad_outputs=torch.ones_like(y),create_graph = \u001b[38;5;28;01mTrue\u001b[39;00m,only_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m#P_yy = torch.autograd.grad(P_y,y,grad_outputs=torch.ones_like(x),create_graph = True,allow_unused = True)[0]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 25.78 GiB is allocated by PyTorch, and 300.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "net1, net2, net3 = main_lbfgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfda07-be96-4a24-96f5-88d9af7bcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "net1.eval()\n",
    "net2.eval()\n",
    "net3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560778-32d8-4a85-8a47-ba46ff6e96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION USING HARD ENFORCEMENT\n",
    "\n",
    "# File path\n",
    "file_path = \"data/unscaled/W_0.55_10_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d, N)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (u, v, p)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Extract individual elements\n",
    "x, y, d = input_tensor[:, 0], input_tensor[:, 1], input_tensor[:, 2]\n",
    "\n",
    "# Reshape x, y, d to match the expected input shape\n",
    "x = x.unsqueeze(-1)  # Shape (B, 1)\n",
    "y = y.unsqueeze(-1)  # Shape (B, 1)\n",
    "d = d.unsqueeze(-1)  # Shape (B, 1)\n",
    "# Concatenate x, y, d to form the input tensor\n",
    "input_tensor = torch.cat((x, y, d), dim=1)  # Shape\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred = net1(input_tensor)\n",
    "    v_pred = net2(input_tensor)\n",
    "    p_pred = net3(input_tensor)\n",
    "\n",
    "n = 10\n",
    "# n = input_tensor[:, 3]\n",
    "\n",
    "# Apply conditions\n",
    "v_pred[x == 0] = 0  # If x = 0, set v_pred to zero\n",
    "\n",
    "# Compute distance\n",
    "distances = compute_distance(x, y, d, n)\n",
    "\n",
    "# If distance is zero, set u and v to zero\n",
    "mask = distances == 0\n",
    "u_pred[mask] = 0\n",
    "v_pred[mask] = 0\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab819-01e5-43e9-b954-4f54c38c145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for NO MODIFICATION\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"data/unscaled/W_0.55_10_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input (same format as training)\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (x, y, d)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Reshape input tensor to match the expected input shape\n",
    "x = input_tensor[:, 0].unsqueeze(-1)  # Shape (B, 1)\n",
    "y = input_tensor[:, 1].unsqueeze(-1)  # Shape (B, 1)\n",
    "d = input_tensor[:, 2].unsqueeze(-1)  # Shape (B, 1)\n",
    "# Concatenate x, y, d to form the input tensor\n",
    "input_tensor = torch.cat((x, y, d), dim=1)  # Shape\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = net1(input_tensor)\n",
    "    v_pred = net2(input_tensor)\n",
    "    p_pred = net3(input_tensor)\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131cda-ad99-4ec3-9f9b-930076f7abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = input_data[:, 0].reshape(-1)\n",
    "y_grid = input_data[:, 1].reshape(-1)\n",
    "v_actual_grid = output_data[:, 1].reshape(-1)\n",
    "v_pred_grid = v_pred.flatten()\n",
    "u_actual_grid = output_data[:, 0].reshape(-1)\n",
    "u_pred_grid = u_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9beeee-c7e0-4bc5-95f2-b8c1f08b41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x_grid,\n",
    "    'y': y_grid,\n",
    "    'u': u_pred_grid,\n",
    "    'v': v_pred_grid\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('results/label_free/csv/LF_0.55_10.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83f0c-41dc-43c0-a508-01e19fa44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ed29b-ed83-4cae-9024-8ef28858d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14fe0d-bef8-44ca-95ca-099e46f48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ece96-a3df-4962-bbac-34c537436f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
