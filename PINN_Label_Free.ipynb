{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "from compute_distance import compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d17dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_n = 60\n",
    "input_n = 4\n",
    "e_idx = -1\n",
    "num_epochs = 500\n",
    "\n",
    "lr = [1e-3, 1e-3, 5e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679a7688-0958-4875-b603-ac0afd60f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net3(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            ################## below are added layers\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da46a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = r\"models\\label_free\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "net1_path = os.path.join(weights_dir, f\"label_free_1_epoch_{e_idx}.pth\")\n",
    "net2_path = os.path.join(weights_dir, f\"label_free_2_epoch_{e_idx}.pth\")\n",
    "net3_path = os.path.join(weights_dir, f\"label_free_3_epoch_{e_idx}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    # Move models to GPU\n",
    "    net1 = Net1().to(device)\n",
    "    net2 = Net2().to(device)\n",
    "    net3 = Net3().to(device)\n",
    "\n",
    "    print(f\"net1 is on: {next(net1.parameters()).device}\")\n",
    "    print(f\"net2 is on: {next(net2.parameters()).device}\")\n",
    "    print(f\"net3 is on: {next(net3.parameters()).device}\")\n",
    "\n",
    "    def init_normal(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # use the modules apply function to recursively apply the initialization   \n",
    "    net1.apply(init_normal)\n",
    "    net2.apply(init_normal)\n",
    "    net3.apply(init_normal)\n",
    "\n",
    "    optimizer1 = optim.Adam(net1.parameters(), lr=lr[0], betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer2\t= optim.Adam(net2.parameters(), lr=lr[1], betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer3\t= optim.Adam(net3.parameters(), lr=lr[2], betas = (0.9,0.99),eps = 10**-15)\n",
    "\n",
    "    if os.path.exists(net1_path):\n",
    "        net1.load_state_dict(torch.load(net1_path))\n",
    "        net1.train()\n",
    "        print(f\"✅ Loaded weights from {net1_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net2_path):\n",
    "        net2.load_state_dict(torch.load(net2_path))\n",
    "        net2.train()\n",
    "        print(f\"✅ Loaded weights from {net2_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net3_path):\n",
    "        net3.load_state_dict(torch.load(net3_path))\n",
    "        net3.train()\n",
    "        print(f\"✅ Loaded weights from {net3_path}\")\n",
    "    else:\n",
    "        e_idx = -1\n",
    "        print(f\"⚠️ No saved weights found at {net3_path}. Training from scratch.\")\n",
    "\n",
    "    return net1, net2, net3, optimizer1, optimizer2, optimizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x_batch, y_batch, net1, net2, net3, mse_loss, pde_weight, v_loss_weight, p_loss_weight, rho=1.0, nu=0.01, ds = 0.4):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - net1, net2, net3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "\n",
    "    x.requires_grad_()\n",
    "    y.requires_grad_()\n",
    "    d.requires_grad_()\n",
    "    n.requires_grad_()\n",
    "\n",
    "    # net_in = torch.cat((x, y, d), dim=1)\n",
    "    net_in = torch.cat((x, y, d, n), dim=1)\n",
    "\n",
    "    u = net1(net_in)\n",
    "    v = net2(net_in)\n",
    "    p = net3(net_in)\n",
    "\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "\n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "\n",
    "    u_hard = u\n",
    "    v_hard = v \n",
    "    p_hard = p\n",
    "\n",
    "    u_avg = 0.01 / d\n",
    "    u_max = (3 / 2) * u_avg\n",
    "\n",
    "    # Instead of in-place modifications:\n",
    "    boundary_condition = (distances < 1e-6)  # Small tolerance\n",
    "    u_hard = torch.where(boundary_condition, torch.zeros_like(u_hard), u_hard)\n",
    "    v_hard = torch.where(boundary_condition, torch.zeros_like(v_hard), v_hard)\n",
    "\n",
    "    inlet_condition = (torch.abs(x) < 1e-6)\n",
    "    u_inlet = u_max * (1 - (4*(((ds / 2) - y) ** 2)) / ((ds - d) ** 2))\n",
    "    u_hard = torch.where(inlet_condition, u_inlet, u_hard)\n",
    "    v_hard = torch.where(inlet_condition, torch.zeros_like(v), v_hard)\n",
    "\n",
    "    u_x = torch.autograd.grad(u_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_y = torch.autograd.grad(u_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    p_x = torch.autograd.grad(p_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    #P_xx = torch.autograd.grad(p_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    loss_1 = (u_hard*u_x+v_hard*u_y-nu*(u_xx+u_yy)+1/rho*p_x)\n",
    "\n",
    "    v_x = torch.autograd.grad(v_hard,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_y = torch.autograd.grad(v_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    P_y = torch.autograd.grad(p_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    #P_yy = torch.autograd.grad(P_y,y,grad_outputs=torch.ones_like(x),create_graph = True,allow_unused = True)[0]\n",
    "\n",
    "\n",
    "    loss_2 = (u_hard*v_x+v_hard*v_y - nu*(v_xx+v_yy)+1/rho*P_y)\n",
    "    #Main_deriv = torch.cat((u_x,u_xx,u_y,u_yy,P_x,v_x,v_xx,v_y,v_yy,P_y),1)\n",
    "    loss_3 = (u_x + v_y)\n",
    "    #loss_3 = u_x**2 + 2*u_y*v_x + v_y**2+1/rho*(P_xx + P_yy)\n",
    "    #loss_3 = loss_3*100\n",
    "\n",
    "    loss_1 = mse_loss(loss_1, torch.zeros_like(loss_1))\n",
    "    loss_2 = mse_loss(loss_2, torch.zeros_like(loss_2))\n",
    "    loss_3 = mse_loss(loss_3, torch.zeros_like(loss_3))\n",
    "\n",
    "    loss_4 = mse_loss(u, y_batch[:, 0:1])\n",
    "    loss_5 = mse_loss(v, y_batch[:, 1:2])\n",
    "    loss_6 = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "    loss = pde_weight * (loss_1 + loss_2 + loss_3) + loss_4 + loss_5 * v_loss_weight + loss_6 * p_loss_weight\n",
    "\n",
    "    return loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c29478-f7a8-40cf-974f-cbb8f04ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss track\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "    # file_path = os.path.join(os.getcwd(), \"data/unscaled/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # Load data using pandas and convert to NumPy array\n",
    "    # df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    # data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "\n",
    "    csv_dir      = os.path.join(os.getcwd(), \"data\", \"unscaled\")\n",
    "    pattern      = os.path.join(csv_dir, \"W_*_*_1.csv\") \n",
    "    csv_paths    = sorted(glob.glob(pattern))           \n",
    "\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "\n",
    "    frames = [pd.read_csv(p) for p in csv_paths]\n",
    "    df     = pd.concat(frames, ignore_index=True)\n",
    "    data        = df.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=100000, shuffle=True)\n",
    "    print(drt)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    net1, net2, net3, optimizer1, optimizer2, optimizer3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    mse_loss = MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # Lists to store loss history\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'batch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': [],\n",
    "        'loss_6': []\n",
    "    }\n",
    "\n",
    "    loss_history_by_epoch = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': [],\n",
    "        'loss_6': []\n",
    "    }\n",
    "\n",
    "    v_loss_weight = 50\n",
    "    p_loss_weight = 1\n",
    "    pde_weight = 0\n",
    "    \n",
    "    for epoch in range(e_idx+1, num_epochs+1):\n",
    "        total_loss, total_loss_1, total_loss_2, total_loss_3, total_loss_4, total_loss_5, total_loss_6 = 0, 0, 0, 0, 0, 0, 0\n",
    "        num_batches = len(dataloader)\n",
    "        batch_idx = 0\n",
    "\n",
    "        if epoch >= 100:\n",
    "            v_loss_weight = 100\n",
    "            pde_weight = 1\n",
    "        \n",
    "        elif epoch >= 150:\n",
    "            pde_weight = 2\n",
    "\n",
    "        elif epoch >= 200:\n",
    "            v_loss_weight = 200\n",
    "            pde_weight = 5\n",
    "        \n",
    "        elif epoch >= 300:\n",
    "            v_loss_weight = 300\n",
    "            pde_weight = 10\n",
    "\n",
    "        elif epoch >= 400:\n",
    "            v_loss_weight = 500\n",
    "            pde_weight = 100\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "            net1.zero_grad()\n",
    "            net2.zero_grad()\n",
    "            net3.zero_grad()\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3, loss_4, loss_5, loss_6 = criterion(x_batch, y_batch, net1, net2, net3, mse_loss, pde_weight, v_loss_weight, p_loss_weight)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "\n",
    "            # Store losses in history\n",
    "            loss_history['epoch'].append(epoch)\n",
    "            loss_history['batch'].append(batch_idx)\n",
    "            loss_history['loss'].append(loss.item())\n",
    "            loss_history['loss_1'].append(loss_1.item())\n",
    "            loss_history['loss_2'].append(loss_2.item())\n",
    "            loss_history['loss_3'].append(loss_3.item())\n",
    "            loss_history['loss_4'].append(loss_4.item())\n",
    "            loss_history['loss_5'].append(loss_5.item())\n",
    "            loss_history['loss_6'].append(loss_6.item())\n",
    "\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_loss_1 += loss_1.item()\n",
    "            total_loss_2 += loss_2.item()\n",
    "            total_loss_3 += loss_3.item()\n",
    "            total_loss_4 += loss_4.item()\n",
    "            total_loss_5 += loss_5.item()\n",
    "            total_loss_6 += loss_6.item()\n",
    "\n",
    "            batch_idx += 1\n",
    "    \n",
    "        # Compute average loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_loss_1 = total_loss_1 / num_batches\n",
    "        avg_loss_2 = total_loss_2 / num_batches\n",
    "        avg_loss_3 = total_loss_3 / num_batches\n",
    "        avg_loss_4 = total_loss_4 / num_batches\n",
    "        avg_loss_5 = total_loss_5 / num_batches\n",
    "        avg_loss_6 = total_loss_6 / num_batches\n",
    "\n",
    "        # Store average losses by epoch\n",
    "        loss_history_by_epoch['epoch'].append(epoch)\n",
    "        loss_history_by_epoch['loss'].append(avg_loss)\n",
    "        loss_history_by_epoch['loss_1'].append(avg_loss_1)\n",
    "        loss_history_by_epoch['loss_2'].append(avg_loss_2)\n",
    "        loss_history_by_epoch['loss_3'].append(avg_loss_3)\n",
    "        loss_history_by_epoch['loss_4'].append(avg_loss_4)\n",
    "        loss_history_by_epoch['loss_5'].append(avg_loss_5)\n",
    "        loss_history_by_epoch['loss_6'].append(avg_loss_6)\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "\n",
    "        elif epoch % 100 == 0:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: Models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg Loss_1: {avg_loss_1:.7f}, \"\n",
    "                  f\"Avg Loss_2: {avg_loss_2:.7f}, Avg Loss_3: {avg_loss_3:.7f}, Avg Loss_4: {avg_loss_4:.7f}, Avg Loss_5: {avg_loss_5:.7f}, Avg Loss_6: {avg_loss_6:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "\n",
    "    loss_csv_path = 'results/label_free/training_losses_label_free.csv'\n",
    "    \n",
    "    if os.path.exists(loss_csv_path):\n",
    "        existing_loss_df = pd.read_csv(loss_csv_path)\n",
    "    else:\n",
    "        existing_loss_df = None\n",
    "\n",
    "    if existing_loss_df is not None:\n",
    "        loss_df = pd.concat([existing_loss_df, loss_df], ignore_index=True)\n",
    "\n",
    "    loss_df.to_csv('results/label_free/training_losses_label_free.csv', index=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss'], label='Total Loss')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_1'], label='Loss 1')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_2'], label='Loss 2')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_3'], label='Loss 3')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_4'], label='Loss 4')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_5'], label='Loss 5')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_6'], label='Loss 6')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/label_free/loss_curves_label_free.png')\n",
    "    plt.show()\n",
    "\n",
    "    return net1, net2, net3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1, net2, net3 = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
