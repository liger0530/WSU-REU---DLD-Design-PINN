{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a7688-0958-4875-b603-ac0afd60f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "h_nD = 30\n",
    "h_n = 20\n",
    "input_n = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class NavierStokesPINN_U(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_U, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NavierStokesPINN_V(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_V, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NavierStokesPINN_P(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_P, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = r\"models\\label_free\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "e_idx = -1\n",
    "net1_path = os.path.join(weights_dir, f\"label_free_1_epoch_{e_idx}.pth\")\n",
    "net2_path = os.path.join(weights_dir, f\"label_free_2_epoch_{e_idx}.pth\")\n",
    "net3_path = os.path.join(weights_dir, f\"label_free_3_epoch_{e_idx}.pth\")\n",
    "\n",
    "def initialize_models():\n",
    "    # Move models to GPU\n",
    "    net1 = NavierStokesPINN_U().to(device)\n",
    "    net2 = NavierStokesPINN_V().to(device)\n",
    "    net3 = NavierStokesPINN_P().to(device)\n",
    "\n",
    "    print(f\"net1 is on: {next(net1.parameters()).device}\")\n",
    "    print(f\"net2 is on: {next(net2.parameters()).device}\")\n",
    "    print(f\"net3 is on: {next(net3.parameters()).device}\")\n",
    "\n",
    "    def init_normal(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # use the modules apply function to recursively apply the initialization   \n",
    "    net1.apply(init_normal)\n",
    "    net2.apply(init_normal)\n",
    "    net3.apply(init_normal)\n",
    "\n",
    "\n",
    "    optimizer1 = optim.Adam(net1.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer2\t= optim.Adam(net2.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "    optimizer3\t= optim.Adam(net3.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\n",
    "    if os.path.exists(net1_path):\n",
    "        net1.load_state_dict(torch.load(net1_path))\n",
    "        net1.train()\n",
    "        print(f\"✅ Loaded weights from {net1_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net2_path):\n",
    "        net2.load_state_dict(torch.load(net2_path))\n",
    "        net2.train()\n",
    "        print(f\"✅ Loaded weights from {net2_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net3_path):\n",
    "        net3.load_state_dict(torch.load(net3_path))\n",
    "        net3.train()\n",
    "        print(f\"✅ Loaded weights from {net3_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net3_path}. Training from scratch.\")\n",
    "\n",
    "    return net1, net2, net3, optimizer1, optimizer2, optimizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(x, y, d, n):\n",
    "    \"\"\"Compute the minimum distance of (x, y) from the nearest circular post.\"\"\"\n",
    "    tilt = 0.4/n\n",
    "    centers = [(0, 0), (0, 0.4), (0.4, tilt), (0.4, 0.4+tilt)]\n",
    "    \n",
    "    distances = torch.full_like(x, float(\"inf\"))  # Initialize distances with large values\n",
    "\n",
    "    for cx, cy in centers:\n",
    "        r = d / 2\n",
    "        distance_to_post = torch.sqrt((x - cx) ** 2 + (y - cy) ** 2) - r\n",
    "        distances = torch.minimum(distances, distance_to_post)  # Take the minimum distance\n",
    "\n",
    "    return torch.maximum(distances, torch.tensor(0.0))  # Set negative distances to zero\n",
    "\n",
    "def criterion(x_batch, y_batch, net1, net2, net3, mse_loss, rho=1.0, nu=0.01):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - net1, net2, net3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "    x.requires_grad_()\n",
    "    y.requires_grad_()\n",
    "    d.requires_grad_()\n",
    "    n.requires_grad_()\n",
    "\n",
    "    net_in = torch.cat((x, y, d, n), dim=1)\n",
    "\n",
    "    u = net1(net_in)\n",
    "    v = net2(net_in)\n",
    "    p = net3(net_in)\n",
    "\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "\n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "    u[distances == 0] = 0\n",
    "\n",
    "    u_hard = u\n",
    "    v_hard = distances * v\n",
    "    p_hard = p\n",
    "\n",
    "    u_x = torch.autograd.grad(u_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    u_y = torch.autograd.grad(u_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    p_x = torch.autograd.grad(p_hard,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    #P_xx = torch.autograd.grad(p_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "    loss_1 = (u_hard*u_x+v_hard*u_y-nu*(u_xx+u_yy)+1/rho*p_x)\n",
    "\n",
    "    v_x = torch.autograd.grad(v_hard,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_y = torch.autograd.grad(v_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    \n",
    "    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    P_y = torch.autograd.grad(p_hard,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "    #P_yy = torch.autograd.grad(P_y,y,grad_outputs=torch.ones_like(x),create_graph = True,allow_unused = True)[0]\n",
    "\n",
    "\n",
    "    loss_2 = (u_hard*v_x+v_hard*v_y - nu*(v_xx+v_yy)+1/rho*P_y)\n",
    "    #Main_deriv = torch.cat((u_x,u_xx,u_y,u_yy,P_x,v_x,v_xx,v_y,v_yy,P_y),1)\n",
    "    loss_3 = (u_x + v_y)\n",
    "    #loss_3 = u_x**2 + 2*u_y*v_x + v_y**2+1/rho*(P_xx + P_yy)\n",
    "    #loss_3 = loss_3*100\n",
    "\n",
    "    loss_1 = mse_loss(loss_1, torch.zeros_like(loss_1))\n",
    "    loss_2 = mse_loss(loss_2, torch.zeros_like(loss_2))\n",
    "    loss_3 = mse_loss(loss_3, torch.zeros_like(loss_3))\n",
    "\n",
    "    loss_4 = mse_loss(u, y_batch[:, 0:1])\n",
    "    loss_5 = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "\n",
    "    loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5\n",
    "\n",
    "    return loss, loss_1, loss_2, loss_3, loss_4, loss_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c29478-f7a8-40cf-974f-cbb8f04ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss track\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "    file_path = os.path.join(os.getcwd(), \"data/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # Load data using pandas and convert to NumPy array\n",
    "    df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=100000, shuffle=True)\n",
    "    print(drt)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    net1, net2, net3, optimizer1, optimizer2, optimizer3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 500\n",
    "    mse_loss = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # Lists to store loss history\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'batch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': []\n",
    "    }\n",
    "\n",
    "    loss_history_by_epoch = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'loss_1': [],\n",
    "        'loss_2': [],\n",
    "        'loss_3': [],\n",
    "        'loss_4': [],\n",
    "        'loss_5': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_loss_1, total_loss_2, total_loss_3, total_loss_4, total_loss_5 = 0, 0, 0, 0, 0, 0\n",
    "        num_batches = len(dataloader)\n",
    "        batch_idx = 0\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "            net1.zero_grad()\n",
    "            net2.zero_grad()\n",
    "            net3.zero_grad()\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3, loss_4, loss_5 = criterion(x_batch, y_batch, net1, net2, net3, mse_loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "\n",
    "            # Store losses in history\n",
    "            loss_history['epoch'].append(epoch)\n",
    "            loss_history['batch'].append(batch_idx)\n",
    "            loss_history['loss'].append(loss.item())\n",
    "            loss_history['loss_1'].append(loss_1.item())\n",
    "            loss_history['loss_2'].append(loss_2.item())\n",
    "            loss_history['loss_3'].append(loss_3.item())\n",
    "            loss_history['loss_4'].append(loss_4.item())\n",
    "            loss_history['loss_5'].append(loss_5.item())\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_loss_1 += loss_1.item()\n",
    "            total_loss_2 += loss_2.item()\n",
    "            total_loss_3 += loss_3.item()\n",
    "            total_loss_4 += loss_4.item()\n",
    "            total_loss_5 += loss_5.item()\n",
    "\n",
    "            batch_idx += 1\n",
    "    \n",
    "        # Compute average loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_loss_1 = total_loss_1 / num_batches\n",
    "        avg_loss_2 = total_loss_2 / num_batches\n",
    "        avg_loss_3 = total_loss_3 / num_batches\n",
    "        avg_loss_4 = total_loss_4 / num_batches\n",
    "        avg_loss_5 = total_loss_5 / num_batches\n",
    "\n",
    "        # Store average losses by epoch\n",
    "        loss_history_by_epoch['epoch'].append(epoch)\n",
    "        loss_history_by_epoch['loss'].append(avg_loss)\n",
    "        loss_history_by_epoch['loss_1'].append(avg_loss_1)\n",
    "        loss_history_by_epoch['loss_2'].append(avg_loss_2)\n",
    "        loss_history_by_epoch['loss_3'].append(avg_loss_3)\n",
    "        loss_history_by_epoch['loss_4'].append(avg_loss_4)\n",
    "        loss_history_by_epoch['loss_5'].append(avg_loss_5)\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(net1.state_dict(), f'models/label_free/label_free_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/label_free/label_free_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/label_free/label_free_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg Loss_1: {avg_loss_1:.7f}, \"\n",
    "                  f\"Avg Loss_2: {avg_loss_2:.7f}, Avg Loss_3: {avg_loss_3:.7f}, Avg Loss_4: {avg_loss_4:.7f}, Avg Loss_5: {avg_loss_5:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "    loss_df.to_csv('results/label_free/training_losses_label_free.csv', index=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss'], label='Total Loss')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_1'], label='Loss 1')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_2'], label='Loss 2')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_3'], label='Loss 3')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_4'], label='Loss 4')\n",
    "    plt.plot(loss_history_by_epoch['epoch'], loss_history_by_epoch['loss_5'], label='Loss 5')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/label_free/loss_curves_label_free.png')\n",
    "    plt.show()\n",
    "\n",
    "    return net1, net2, net3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1, net2, net3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfda07-be96-4a24-96f5-88d9af7bcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "net1.eval()\n",
    "net2.eval()\n",
    "net3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560778-32d8-4a85-8a47-ba46ff6e96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION USING HARD ENFORCEMENT\n",
    "\n",
    "# File path\n",
    "file_path = \"data/csv/W_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d, N)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (u, v, p)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define function to compute the minimum distance from the nearest circular post\n",
    "def compute_distance(x, y, d, n):\n",
    "    \"\"\"Compute the minimum distance of (x, y) from the nearest circular post.\"\"\"\n",
    "    tilt = 0.4 / n\n",
    "    centers = [(0, 0), (0, 0.4), (0.4, tilt), (0.4, 0.4 + tilt)]\n",
    "    \n",
    "    distances = torch.full_like(x, float(\"inf\"))  # Initialize distances with large values\n",
    "\n",
    "    for cx, cy in centers:\n",
    "        r = d / 2\n",
    "        distance_to_post = torch.sqrt((x - cx) ** 2 + (y - cy) ** 2) - r\n",
    "        distances = torch.minimum(distances, distance_to_post)  # Take the minimum distance\n",
    "\n",
    "    return torch.maximum(distances, torch.tensor(0.0, device=device))  # Set negative distances to zero\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred = net1(input_tensor)\n",
    "    v_pred = net2(input_tensor)\n",
    "    p_pred = net3(input_tensor)\n",
    "\n",
    "# Extract individual elements\n",
    "x, y, d, n = input_tensor[:, 0], input_tensor[:, 1], input_tensor[:, 2], input_tensor[:, 3]\n",
    "\n",
    "# Apply conditions\n",
    "v_pred[x == 0] = 0  # If x = 0, set v_pred to zero\n",
    "\n",
    "# Compute distance\n",
    "distances = compute_distance(x, y, d, n)\n",
    "\n",
    "# If distance is zero, set u and v to zero\n",
    "mask = distances == 0\n",
    "u_pred[mask] = 0\n",
    "v_pred[mask] = 0\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab819-01e5-43e9-b954-4f54c38c145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for NO MODIFICATION\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"data/csv/W_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input (same format as training)\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (x, y, d)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = net1(input_tensor)\n",
    "    v_pred = net2(input_tensor)\n",
    "    p_pred = net3(input_tensor)\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131cda-ad99-4ec3-9f9b-930076f7abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = input_data[:, 0].reshape(-1)\n",
    "y_grid = input_data[:, 1].reshape(-1)\n",
    "v_actual_grid = output_data[:, 1].reshape(-1)\n",
    "v_pred_grid = v_pred.flatten()\n",
    "u_actual_grid = output_data[:, 0].reshape(-1)\n",
    "u_pred_grid = u_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9beeee-c7e0-4bc5-95f2-b8c1f08b41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x_grid,\n",
    "    'y': y_grid,\n",
    "    'u': u_pred_grid,\n",
    "    'v': v_pred_grid\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('results/label_free/csv/LF_0.55_14.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83f0c-41dc-43c0-a508-01e19fa44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ed29b-ed83-4cae-9024-8ef28858d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14fe0d-bef8-44ca-95ca-099e46f48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ece96-a3df-4962-bbac-34c537436f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
