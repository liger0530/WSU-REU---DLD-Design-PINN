{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "from compute_distance import compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679a7688-0958-4875-b603-ac0afd60f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "h_nD = 30\n",
    "h_n = 20\n",
    "input_n = 4\n",
    "learning_rate = 5e-4\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output\n",
    "\n",
    "class Net3(nn.Module):\n",
    "\n",
    "    #The __init__ function stack the layers of the \n",
    "    #network Sequentially \n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            nn.Linear(h_n,h_n),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Sigmoid(),\n",
    "            Swish(),\n",
    "            ################## below are added layers\n",
    "\n",
    "            nn.Linear(h_n,1),\n",
    "        )\n",
    "    #This function defines the forward rule of\n",
    "    #output respect to input.\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827ab7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = r\"models\\original\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "e_idx = -1\n",
    "num_epochs = 500\n",
    "net1_path = os.path.join(weights_dir, f\"original_1_epoch_{e_idx}.pth\")\n",
    "net2_path = os.path.join(weights_dir, f\"original_2_epoch_{e_idx}.pth\")\n",
    "net3_path = os.path.join(weights_dir, f\"original_3_epoch_{e_idx}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models_lbfgs():\n",
    "    # Move models to GPU\n",
    "    net1 = Net1().to(device)\n",
    "    net2 = Net2().to(device)\n",
    "    net3 = Net3().to(device)\n",
    "\n",
    "    print(f\"net1 is on: {next(net1.parameters()).device}\")\n",
    "    print(f\"net2 is on: {next(net2.parameters()).device}\")\n",
    "    print(f\"net3 is on: {next(net3.parameters()).device}\")\n",
    "\n",
    "    def init_normal(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # Initialize weights\n",
    "    net1.apply(init_normal)\n",
    "    net2.apply(init_normal)\n",
    "    net3.apply(init_normal)\n",
    "\n",
    "    # Load saved weights (if any)\n",
    "    if os.path.exists(net1_path):\n",
    "        net1.load_state_dict(torch.load(net1_path))\n",
    "        net1.train()\n",
    "        print(f\"✅ Loaded weights from {net1_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net2_path):\n",
    "        net2.load_state_dict(torch.load(net2_path))\n",
    "        net2.train()\n",
    "        print(f\"✅ Loaded weights from {net2_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(net3_path):\n",
    "        net3.load_state_dict(torch.load(net3_path))\n",
    "        net3.train()\n",
    "        print(f\"✅ Loaded weights from {net3_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {net3_path}. Training from scratch.\")\n",
    "\n",
    "    # ✅ Combine all parameters into one LBFGS optimizer\n",
    "    optimizer = optim.LBFGS(\n",
    "        list(net1.parameters()) + list(net2.parameters()) + list(net3.parameters()),\n",
    "        lr=1.0,\n",
    "        max_iter=50,\n",
    "        history_size=50,\n",
    "        tolerance_grad=1e-7,\n",
    "        line_search_fn='strong_wolfe'\n",
    "    )\n",
    "\n",
    "    return net1, net2, net3, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_relative_mse(pred, target, scale=1.0, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes mean squared relative error, scaled for balance across loss terms.\n",
    "    \"\"\"\n",
    "    relative_error = (pred - target) / (target.abs() + eps)\n",
    "    return torch.mean(relative_error ** 2) / scale\n",
    "\n",
    "def criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss, v_loss_weight, p_loss_weight, rho=1.0, nu=0.01):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch: Input tensor (batch of input values)\n",
    "    - y_batch: Target tensor (batch of true output values)\n",
    "    - model_1, model_2, model_3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    # Compute model predictions inside criterion\n",
    "    x, y, d, n = x_batch[:, 0:1], x_batch[:, 1:2], x_batch[:, 2:3], x_batch[:, 3:4]\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    y.requires_grad_(True)\n",
    "    d.requires_grad_(True)\n",
    "    n.requires_grad_(True)\n",
    "\n",
    "    model_inputs = torch.cat((x, y, d, n), dim=1)  # Concatenate inputs for the models\n",
    "\n",
    "    # some sort of ways to actually define the pairs inside of these 50000 points.\n",
    "    u = model_1(model_inputs)\n",
    "    v = model_2(model_inputs)\n",
    "    p = model_3(model_inputs)\n",
    "    #print(u.shape)\n",
    "    u = u.view(len(u),-1)\n",
    "    v = v.view(len(v),-1)\n",
    "    p = p.view(len(p),-1)\n",
    "    #print(u.shape)\n",
    "    \n",
    "    # Compute distance to the nearest circular post\n",
    "    distances = compute_distance(x, y, d, n)\n",
    "    u[distances == 0] = 0\n",
    "    v[distances == 0] = 0\n",
    "    v[x == 0] = 0  #inlet y velocity\n",
    "    p[x == 0.4] = 0\n",
    "    u_loss = mse_loss(u, y_batch[:, 0:1])\n",
    "    v_loss = mse_loss(v, y_batch[:, 1:2])\n",
    "    p_loss = mse_loss(p, y_batch[:, 2:3])\n",
    "\n",
    "\n",
    "    # Compute data loss (MSE against target values)\n",
    "    L_data = u_loss + v_loss + p_loss\n",
    "    #L_data_periodic = ( u_p_loss + v_loss * 500 + p_loss )\n",
    "\n",
    "    # l_data_periodic + L\n",
    "\n",
    "\n",
    "    return L_data, u_loss, v_loss, p_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_lbfgs():\n",
    "    from torch.nn import MSELoss\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load all training data (full-batch)\n",
    "    csv_dir   = os.path.join(os.getcwd(), \"data\", \"unscaled\")\n",
    "    pattern   = os.path.join(csv_dir, \"W_*_*_1.csv\")\n",
    "    csv_paths = sorted(glob.glob(pattern))\n",
    "\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "\n",
    "    frames = [pd.read_csv(p) for p in csv_paths]\n",
    "    df     = pd.concat(frames, ignore_index=True)\n",
    "    data   = df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    dataset = CustomDataset(data)\n",
    "    x_batch, y_batch = dataset[:]  # full batch\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "    # Load models\n",
    "    net1, net2, net3, optimizer = initialize_models_lbfgs()\n",
    "    mse_loss = MSELoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Logging storage\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'total_loss': [],\n",
    "        'u_loss': [],\n",
    "        'v_loss': [],\n",
    "        'p_loss': []\n",
    "    }\n",
    "\n",
    "    v_loss_weight = 1  # Initial weight for v_loss\n",
    "    p_loss_weight = 1\n",
    "    min_loss = float('inf')\n",
    "\n",
    "    latest_losses = {\n",
    "        'total': 0, \n",
    "        'u': 0, \n",
    "        'v': 0, \n",
    "        'p': 0\n",
    "    }\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss, u_loss, v_loss, p_loss = criterion(\n",
    "            x_batch, y_batch, net1, net2, net3, mse_loss, v_loss_weight, p_loss_weight)\n",
    "        loss.backward()\n",
    "\n",
    "        # Store latest breakdown for logging\n",
    "        latest_losses['total'] = loss\n",
    "        latest_losses['u'] = u_loss\n",
    "        latest_losses['v'] = v_loss\n",
    "        latest_losses['p'] = p_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        # if epoch > 100:\n",
    "        #     v_loss_weight = 50\n",
    "\n",
    "        # if epoch > 200:\n",
    "        #     v_loss_weight = 100\n",
    "        \n",
    "        # if epoch > 300:\n",
    "        #     v_loss_weight = 200\n",
    "\n",
    "        # if epoch > 400:\n",
    "        #     v_loss_weight = 500\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        loss = latest_losses['total'].item()\n",
    "        u_loss = latest_losses['u'].item()\n",
    "        v_loss = latest_losses['v'].item()\n",
    "        p_loss = latest_losses['p'].item()\n",
    "\n",
    "        loss_history['epoch'].append(epoch)\n",
    "        loss_history['total_loss'].append(loss)\n",
    "        loss_history['u_loss'].append(u_loss)\n",
    "        loss_history['v_loss'].append(v_loss)\n",
    "        loss_history['p_loss'].append(p_loss)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[{epoch}/{num_epochs}] Total Loss: {loss:.7f} | u_loss: {u_loss:.7f}, v_loss: {v_loss:.7f}, p_loss: {p_loss:.7f} | Time: {elapsed_time:.2f}s\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save(net1.state_dict(), f'models/original/original_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/original/original_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/original/original_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "\n",
    "        elif epoch % 100 == 0:\n",
    "            min_loss = loss.item()\n",
    "            # Save models every 100 epochs\n",
    "            torch.save(net1.state_dict(), f'models/original/original_1_epoch_{epoch}.pth')\n",
    "            torch.save(net2.state_dict(), f'models/original/original_2_epoch_{epoch}.pth')\n",
    "            torch.save(net3.state_dict(), f'models/original/original_3_epoch_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: Models saved\")\n",
    "            \n",
    "    # Save loss log\n",
    "    os.makedirs(\"results/original\", exist_ok=True)\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "    loss_df.to_csv(\"results/original/losses_lbfgs.csv\", index=False)\n",
    "\n",
    "    # Save loss plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for k in loss_history:\n",
    "        if k != 'epoch':\n",
    "            plt.plot(loss_history['epoch'], loss_history[k], label=k)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"LBFGS Losses (Log Scale)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/original/loss_curve_lbfgs.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return net1, net2, net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1 is on: cuda:0\n",
      "net2 is on: cuda:0\n",
      "net3 is on: cuda:0\n",
      "⚠️ No saved weights found at models\\original\\original_1_epoch_-1.pth. Training from scratch.\n",
      "⚠️ No saved weights found at models\\original\\original_2_epoch_-1.pth. Training from scratch.\n",
      "⚠️ No saved weights found at models\\original\\original_3_epoch_-1.pth. Training from scratch.\n",
      "[0/500] Total Loss: 0.0059951 | u_loss: 0.0016425, v_loss: 0.0034388, p_loss: 0.0009137 | Time: 12.38s\n",
      "Epoch 0: New minimum loss 0.0059951, models saved\n",
      "[1/500] Total Loss: 0.0014607 | u_loss: 0.0009891, v_loss: 0.0002020, p_loss: 0.0002697 | Time: 12.51s\n",
      "Epoch 1: New minimum loss 0.0014607, models saved\n",
      "[2/500] Total Loss: 0.0010131 | u_loss: 0.0008324, v_loss: 0.0000990, p_loss: 0.0000817 | Time: 12.24s\n",
      "Epoch 2: New minimum loss 0.0010131, models saved\n",
      "[3/500] Total Loss: 0.0008982 | u_loss: 0.0007297, v_loss: 0.0000769, p_loss: 0.0000916 | Time: 12.90s\n",
      "Epoch 3: New minimum loss 0.0008982, models saved\n",
      "[4/500] Total Loss: 0.0008473 | u_loss: 0.0006983, v_loss: 0.0000737, p_loss: 0.0000753 | Time: 13.05s\n",
      "Epoch 4: New minimum loss 0.0008473, models saved\n",
      "[5/500] Total Loss: 0.0008254 | u_loss: 0.0006949, v_loss: 0.0000672, p_loss: 0.0000633 | Time: 13.62s\n",
      "Epoch 5: New minimum loss 0.0008254, models saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_1, model_2, model_3 = \u001b[43mmain_lbfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mmain_lbfgs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# if epoch > 100:\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m#     v_loss_weight = 50\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# if epoch > 400:\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m#     v_loss_weight = 500\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     loss = latest_losses[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m].item()\n\u001b[32m     76\u001b[39m     u_loss = latest_losses[\u001b[33m'\u001b[39m\u001b[33mu\u001b[39m\u001b[33m'\u001b[39m].item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lbfgs.py:444\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m    442\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._directional_evaluate(closure, x, t, d)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     loss, flat_grad, t, ls_func_evals = \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m    448\u001b[39m opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lbfgs.py:48\u001b[39m, in \u001b[36m_strong_wolfe\u001b[39m\u001b[34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[39m\n\u001b[32m     46\u001b[39m g = g.clone(memory_format=torch.contiguous_format)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m f_new, g_new = \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m ls_func_evals = \u001b[32m1\u001b[39m\n\u001b[32m     50\u001b[39m gtd_new = g_new.dot(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lbfgs.py:442\u001b[39m, in \u001b[36mLBFGS.step.<locals>.obj_func\u001b[39m\u001b[34m(x, t, d)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leeyo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lbfgs.py:296\u001b[39m, in \u001b[36mLBFGS._directional_evaluate\u001b[39m\u001b[34m(self, closure, x, t, d)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     loss = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_param(x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_1, model_2, model_3 = main_lbfgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560778-32d8-4a85-8a47-ba46ff6e96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION USING HARD ENFORCEMENT\n",
    "\n",
    "# File path\n",
    "file_path = \"data/scaled/DRPINN_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d, N)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (u, v, p)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Extract individual elements\n",
    "x, y, d, n = input_tensor[:, 0], input_tensor[:, 1], input_tensor[:, 2], input_tensor[:, 3]\n",
    "\n",
    "# Apply conditions\n",
    "v_pred[x == 0] = 0  # If x = 0, set v_pred to zero\n",
    "\n",
    "# Compute distance\n",
    "distances = compute_distance(x, y, d, n)\n",
    "\n",
    "# If distance is zero, set u and v to zero\n",
    "mask = distances == 0\n",
    "u_pred[mask] = 0\n",
    "v_pred[mask] = 0\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab819-01e5-43e9-b954-4f54c38c145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for NO MODIFICATION\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"data/scaled/W_0.55_14_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input (same format as training)\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (x, y, d)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131cda-ad99-4ec3-9f9b-930076f7abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = input_data[:, 0].reshape(-1)\n",
    "y_grid = input_data[:, 1].reshape(-1)\n",
    "v_actual_grid = output_data[:, 1].reshape(-1)\n",
    "v_pred_grid = v_pred.flatten()\n",
    "u_actual_grid = output_data[:, 0].reshape(-1)\n",
    "u_pred_grid = u_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9beeee-c7e0-4bc5-95f2-b8c1f08b41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x_grid,\n",
    "    'y': y_grid,\n",
    "    'u': u_pred_grid,\n",
    "    'v': v_pred_grid\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('results/original/csv/O_0.55_14.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83f0c-41dc-43c0-a508-01e19fa44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ed29b-ed83-4cae-9024-8ef28858d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14fe0d-bef8-44ca-95ca-099e46f48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ece96-a3df-4962-bbac-34c537436f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
