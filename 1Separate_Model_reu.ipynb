{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f883a1-643f-4a7b-be10-8c44922ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679a7688-0958-4875-b603-ac0afd60f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inplace:\n",
    "            x.mul_(torch.sigmoid(x))\n",
    "            return x\n",
    "        else:\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class NavierStokesPINN_U(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_U, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NavierStokesPINN_V(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_V, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NavierStokesPINN_P(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NavierStokesPINN_P, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            Swish(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b67b3c5-e3ea-466b-a10d-c160d795740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.inputs = torch.tensor(data[:, [0, 1, 5, 7]], dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(data[:, [2, 3, 4]], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b85fea-8189-4b59-8532-198938569e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define absolute paths for saved weights\n",
    "weights_dir = \"models\"  # Use raw string (r\"...\") to avoid escape character issues\n",
    "model_1_path = os.path.join(weights_dir, \"b_329_1_epoch1_5.pth\")\n",
    "model_2_path = os.path.join(weights_dir, \"b_329_2_epoch1_5.pth\")\n",
    "model_3_path = os.path.join(weights_dir, \"b_329_3_epoch1_5.pth\")\n",
    "\n",
    "def initialize_models():\n",
    "    # Move models to GPU\n",
    "    model_1 = NavierStokesPINN_U().to(device)\n",
    "    model_2 = NavierStokesPINN_V().to(device)\n",
    "    model_3 = NavierStokesPINN_P().to(device)\n",
    "\n",
    "    print(f\"Model_1 is on: {next(model_1.parameters()).device}\")\n",
    "    print(f\"Model_2 is on: {next(model_2.parameters()).device}\")\n",
    "    print(f\"Model_3 is on: {next(model_3.parameters()).device}\")\n",
    "\n",
    "\n",
    "    optimizer_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "    optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "    optimizer_3 = optim.Adam(model_3.parameters(), lr=1e-3)\n",
    "\n",
    "        # Load saved weights if they exist\n",
    "    if os.path.exists(model_1_path):\n",
    "        model_1.load_state_dict(torch.load(model_1_path))\n",
    "        model_1.train()\n",
    "        print(f\"✅ Loaded weights from {model_1_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_1_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(model_2_path):\n",
    "        model_2.load_state_dict(torch.load(model_2_path))\n",
    "        model_2.train()\n",
    "        print(f\"✅ Loaded weights from {model_2_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_2_path}. Training from scratch.\")\n",
    "\n",
    "    if os.path.exists(model_3_path):\n",
    "        model_3.load_state_dict(torch.load(model_3_path))\n",
    "        model_3.train()\n",
    "        print(f\"✅ Loaded weights from {model_3_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No saved weights found at {model_3_path}. Training from scratch.\")\n",
    "\n",
    "    return model_1, model_2, model_3, optimizer_1, optimizer_2, optimizer_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed679a-e90f-4cb0-9f49-a89906795846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(x, y, d, n):\n",
    "    \"\"\"Compute the minimum distance of (x, y) from the nearest circular post.\"\"\"\n",
    "    tilt = 0.4/n\n",
    "    centers = [(0, 0), (0, 0.4), (0.4, tilt), (0.4, 0.4+tilt)]\n",
    "    \n",
    "    distances = torch.full_like(x, float(\"inf\"))  # Initialize distances with large values\n",
    "\n",
    "    for cx, cy in centers:\n",
    "        r = d / 2\n",
    "        distance_to_post = torch.sqrt((x - cx) ** 2 + (y - cy) ** 2) - r\n",
    "        distances = torch.minimum(distances, distance_to_post)  # Take the minimum distance\n",
    "\n",
    "    return torch.maximum(distances, torch.tensor(0.0, device=x.device))  # Set negative distances to zero\n",
    "\n",
    "def criterion_pde(x_batch, model_1, model_2, model_3, mse_loss, rho=1.0, nu=0.01):\n",
    "    x_batch.requires_grad_()\n",
    "\n",
    "    u = model_1(x_batch)\n",
    "    v = model_2(x_batch)\n",
    "    p = model_3(x_batch)\n",
    "\n",
    "    # First-order derivatives\n",
    "    du_d = torch.autograd.grad(u, x_batch, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    dv_d = torch.autograd.grad(v, x_batch, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    dp_d = torch.autograd.grad(p, x_batch, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    du_dx, du_dy = du_d[:, 0], du_d[:, 1]\n",
    "    dv_dx, dv_dy = dv_d[:, 0], dv_d[:, 1]\n",
    "    dp_dx, dp_dy = dp_d[:, 0], dp_d[:, 1]\n",
    "\n",
    "    # Second-order derivatives\n",
    "    du_dxx = torch.autograd.grad(du_dx, x_batch, grad_outputs=torch.ones_like(du_dx), create_graph=True)[0][:, 0]\n",
    "    du_dyy = torch.autograd.grad(du_dy, x_batch, grad_outputs=torch.ones_like(du_dy), create_graph=True)[0][:, 1]\n",
    "    dv_dxx = torch.autograd.grad(dv_dx, x_batch, grad_outputs=torch.ones_like(dv_dx), create_graph=True)[0][:, 0]\n",
    "    dv_dyy = torch.autograd.grad(dv_dy, x_batch, grad_outputs=torch.ones_like(dv_dy), create_graph=True)[0][:, 1]\n",
    "\n",
    "    # Navier-Stokes residuals\n",
    "    # Continuity equation\n",
    "    f_c = du_dx + dv_dy\n",
    "\n",
    "    # Momentum equations\n",
    "    f_u = u.squeeze() * du_dx + v.squeeze() * du_dy + (1/rho) * dp_dx - nu * (du_dxx + du_dyy)\n",
    "    f_v = u.squeeze() * dv_dx + v.squeeze() * dv_dy + (1/rho) * dp_dy - nu * (dv_dxx + dv_dyy)\n",
    "\n",
    "    # Loss for each PDE\n",
    "    loss_c = mse_loss(f_c, torch.zeros_like(f_c))\n",
    "    loss_u = mse_loss(f_u, torch.zeros_like(f_u))\n",
    "    loss_v = mse_loss(f_v, torch.zeros_like(f_v))\n",
    "\n",
    "    return loss_c, loss_u, loss_v\n",
    "\n",
    "def criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss, rho=1.0, nu=0.01):\n",
    "    u_pred = model_1(x_batch)\n",
    "    v_pred = model_2(x_batch)\n",
    "    p_pred = model_3(x_batch)\n",
    "\n",
    "    # Data loss\n",
    "    u_loss = mse_loss(u_pred, y_batch[:, 0:1])\n",
    "    v_loss = mse_loss(v_pred, y_batch[:, 1:2])\n",
    "    p_loss = mse_loss(p_pred, y_batch[:, 2:3])\n",
    "    L_data = u_loss + v_loss + p_loss\n",
    "\n",
    "    return L_data, u_loss, v_loss, p_loss\n",
    "\n",
    "def criterion_p(x_batch_periodic, y_batch_periodic, model_1, model_2, model_3, mse_loss, periodic_drt, rho=1.0, nu=0.01):\n",
    "    \"\"\"\n",
    "    Computes the loss based on physics-based PDE constraints and data loss.\n",
    "\n",
    "    Args:\n",
    "    - x_batch_p: Input tensor pairs of periodic conditions (batch of input values)\n",
    "    - y_batch_p: Target tensor pairs of periodic conditions(batch of true output values)\n",
    "    - model_1, model_2, model_3: Neural network models for u, v, and p\n",
    "    - mse_loss: MSE loss function\n",
    "    - rho: Density parameter for PDE constraints\n",
    "    - nu: Viscosity parameter for PDE constraints\n",
    "\n",
    "    Returns:\n",
    "    - Total loss: Sum of physics loss and data loss\n",
    "    \"\"\"\n",
    "    B = x_batch_periodic.shape[0]\n",
    "\n",
    "    x_flat = x_batch_periodic.view(B*2, 4).requires_grad_()\n",
    "    y_flat = y_batch_periodic   .view(B*2, 3)\n",
    "\n",
    "    u_flat = model_1(x_flat).view(-1)\n",
    "    v_flat = model_2(x_flat).view(-1)\n",
    "    p_flat = model_3(x_flat).view(-1)\n",
    "\n",
    "    x_coord = x_flat[:, 0:1]\n",
    "    y_coord = x_flat[:, 1:2]\n",
    "    d      = x_flat[:, 2:3]\n",
    "    n      = x_flat[:, 3:4]\n",
    "\n",
    "    distances = compute_distance(x_coord, y_coord, d, n)\n",
    "    zero_mask = distances == 0\n",
    "\n",
    "    u_flat = torch.where(zero_mask.view(-1), 0.0, u_flat)\n",
    "    v_flat = torch.where(zero_mask.view(-1), 0.0, v_flat)\n",
    "\n",
    "    v_flat = torch.where(x_coord.view(-1) == 0,   0.0, v_flat)\n",
    "    p_flat = torch.where(x_coord.view(-1) == 0.4, 0.0, p_flat)\n",
    "\n",
    "    u_pred = u_flat.view(B, 2)\n",
    "    v_pred = v_flat.view(B, 2)\n",
    "    p_pred = p_flat.view(B, 2)\n",
    "\n",
    "    u_diff_loss = mse_loss(u_pred[:,0] - u_pred[:,1],\n",
    "                           torch.zeros(B, device=u_pred.device))\n",
    "    v_diff_loss = mse_loss(v_pred[:,0] - v_pred[:,1],\n",
    "                           torch.zeros(B, device=v_pred.device))\n",
    "    p_diff_loss = mse_loss(p_pred[:,0] - p_pred[:,1],\n",
    "                           torch.zeros(B, device=p_pred.device))\n",
    "\n",
    "    L_diff = periodic_drt * (u_diff_loss + v_diff_loss + p_diff_loss)\n",
    "\n",
    "    u_data_loss = mse_loss(u_flat, y_flat[:, 0])\n",
    "    v_data_loss = mse_loss(v_flat, y_flat[:, 1])\n",
    "    p_data_loss = mse_loss(p_flat, y_flat[:, 2])\n",
    "\n",
    "    L_data = u_data_loss + v_data_loss + p_data_loss\n",
    "\n",
    "    return L_data, L_diff, u_data_loss, v_data_loss, p_data_loss, \\\n",
    "           u_diff_loss, v_diff_loss, p_diff_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd89db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_p(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: array-like of shape (N, 2, 8)\n",
    "        \"\"\"\n",
    "        # convert to float tensor if it isn't already\n",
    "        self.data = torch.as_tensor(data, dtype=torch.float32)\n",
    "        # specify which columns are inputs/targets\n",
    "        self.input_idx  = [0, 1, 5, 7]\n",
    "        self.target_idx = [2, 3, 4]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # sample: shape (2, 8)\n",
    "        sample = self.data[idx]\n",
    "        # inputs: take cols 0,1,5,7 → (2,4)\n",
    "        x = sample[:, self.input_idx]\n",
    "        # targets: take cols 2,3,4 → (2,3)\n",
    "        y = sample[:, self.target_idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c29478-f7a8-40cf-974f-cbb8f04ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#loss track\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "    file_path = os.path.join(os.getcwd(), \"data/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # Load data using pandas and convert to NumPy array\n",
    "    df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "\n",
    "    upper = 16500\n",
    "    size = 18000\n",
    "    num_configs = 120\n",
    "    periodic_data = []\n",
    "\n",
    "    for i in range(num_configs):\n",
    "        for j in range(i*size + upper, i*size + upper + 250):\n",
    "            periodic_data.append((data[j], data[j+250]))\n",
    "\n",
    "    periodic_dataset = CustomDataset_p(np.array(periodic_data))\n",
    "\n",
    "    periodic_drt = len(periodic_dataset)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=100000, shuffle=True)\n",
    "    periodic_dataloader = DataLoader(periodic_dataset, batch_size=30000, shuffle=True)\n",
    "\n",
    "    print(drt)\n",
    "    print(periodic_drt)\n",
    "    num_batches = len(dataloader) + len(periodic_dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    model_1, model_2, model_3, optimizer_1, optimizer_2, optimizer_3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    mse_loss = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # Lists to store loss history\n",
    "    loss_history = {\n",
    "        'epoch': [],\n",
    "        'total_loss': [],\n",
    "        'u_loss': [],\n",
    "        'v_loss': [],\n",
    "        'p_loss': [],\n",
    "        # periodic losses\n",
    "        'periodic_total_loss': [],\n",
    "        'u_diff_loss': [],\n",
    "        'v_diff_loss': [],\n",
    "        'p_diff_loss': [],\n",
    "        'periodic_diff_total': []\n",
    "    }\n",
    "    \n",
    "    # Loss weights\n",
    "    lambda_data = 1.0\n",
    "    lambda_pde = 0.1\n",
    "    lambda_periodic = 1.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_u_loss, total_v_loss, total_p_loss = 0, 0, 0, 0\n",
    "        total_periodic_loss, sum_u_diff, sum_v_diff, sum_p_diff = 0, 0, 0, 0\n",
    "        total_pde_loss = 0\n",
    "\n",
    "        # Create iterators\n",
    "        data_iter = iter(dataloader)\n",
    "        periodic_iter = iter(periodic_dataloader)\n",
    "        \n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            # PDE loss (using a subset of data as collocation points)\n",
    "            x_batch, y_batch = next(data_iter)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss_c, loss_u_pde, loss_v_pde = criterion_pde(x_batch, model_1, model_2, model_3, mse_loss)\n",
    "            L_pde = loss_c + loss_u_pde + loss_v_pde\n",
    "            \n",
    "            # Data loss\n",
    "            L_data, u_loss, v_loss, p_loss = criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss)\n",
    "            \n",
    "            # Periodic loss\n",
    "            try:\n",
    "                x_batch_periodic, y_batch_periodic = next(periodic_iter)\n",
    "            except StopIteration:\n",
    "                periodic_iter = iter(periodic_dataloader)\n",
    "                x_batch_periodic, y_batch_periodic = next(periodic_iter)\n",
    "            x_batch_periodic, y_batch_periodic = x_batch_periodic.to(device), y_batch_periodic.to(device)\n",
    "            L_data_p, L_diff_p, u_loss_p, v_loss_p, p_loss_p, u_diff_p, v_diff_p, p_diff_p = criterion_p(\n",
    "                                x_batch_periodic, y_batch_periodic, model_1, model_2, model_3, mse_loss, periodic_drt)\n",
    "            loss_periodic = L_data_p + L_diff_p\n",
    "\n",
    "            # Total loss\n",
    "            loss = lambda_data * L_data + lambda_pde * L_pde + lambda_periodic * loss_periodic\n",
    "            \n",
    "            optimizer_1.zero_grad()\n",
    "            optimizer_2.zero_grad()\n",
    "            optimizer_3.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_1.step()\n",
    "            optimizer_2.step()\n",
    "            optimizer_3.step()\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_u_loss += u_loss.item()\n",
    "            total_v_loss += v_loss.item()\n",
    "            total_p_loss += p_loss.item()\n",
    "            total_pde_loss += L_pde.item()\n",
    "            total_periodic_loss += loss_periodic.item()\n",
    "            sum_u_diff += u_diff_p.item()\n",
    "            sum_v_diff += v_diff_p.item()\n",
    "            sum_p_diff += p_diff_p.item()\n",
    "    \n",
    "        # Compute average losses\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_u_loss = total_u_loss / num_batches\n",
    "        avg_v_loss = total_v_loss / num_batches\n",
    "        avg_p_loss = total_p_loss / num_batches\n",
    "        avg_pde_loss = total_pde_loss / num_batches\n",
    "\n",
    "        avg_periodic_loss = total_periodic_loss / num_batches\n",
    "        avg_u_diff = sum_u_diff / len(periodic_dataloader)\n",
    "        avg_v_diff = sum_v_diff / len(periodic_dataloader)\n",
    "        avg_p_diff = sum_p_diff / len(periodic_dataloader)\n",
    "        avg_diff_total = avg_u_diff + avg_v_diff + avg_p_diff\n",
    "        \n",
    "        # Store losses in history\n",
    "        loss_history['epoch'].append(epoch)\n",
    "        loss_history['total_loss'].append(avg_loss)\n",
    "        loss_history['u_loss'].append(avg_u_loss)\n",
    "        loss_history['v_loss'].append(avg_v_loss)\n",
    "        loss_history['p_loss'].append(avg_p_loss)\n",
    "        loss_history['periodic_total_loss'].append(avg_periodic_loss)\n",
    "        loss_history['u_diff_loss'].append(avg_u_diff)\n",
    "        loss_history['v_diff_loss'].append(avg_v_diff)\n",
    "        loss_history['p_diff_loss'].append(avg_p_diff)\n",
    "        loss_history['periodic_diff_total'].append(avg_diff_total)\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(model_1.state_dict(), f'models/b_329_1_epoch1_{epoch}.pth')\n",
    "            torch.save(model_2.state_dict(), f'models/b_329_2_epoch1_{epoch}.pth')\n",
    "            torch.save(model_3.state_dict(), f'models/b_329_3_epoch1_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg PDE Loss: {avg_pde_loss:.7f}, \"\n",
    "                  f\"Avg Periodic: {avg_periodic_loss:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_df = pd.DataFrame(loss_history)\n",
    "    loss_df.to_csv('training_losses_wang.csv', index=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(loss_history['epoch'], loss_history['total_loss'], label='Total Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['u_loss'], label='U Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['v_loss'], label='V Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['p_loss'], label='P Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['periodic_total_loss'], label='Periodic Total Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['periodic_diff_total'], label='Periodic Diff Total')\n",
    "    plt.plot(loss_history['epoch'], loss_history['u_diff_loss'], label='U Diff Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['v_diff_loss'], label='V Diff Loss')\n",
    "    plt.plot(loss_history['epoch'], loss_history['p_diff_loss'], label='P Diff Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curves_wang.png')\n",
    "    plt.show()\n",
    "\n",
    "    return model_1, model_2, model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c19ba7-ff07-41a5-b815-d9b7b347ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1, model_2, model_3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfda07-be96-4a24-96f5-88d9af7bcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560778-32d8-4a85-8a47-ba46ff6e96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION USING HARD ENFORCEMENT\n",
    "\n",
    "# File path\n",
    "file_path = \"data/csv/DRPINN_0.3_3_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d, N)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (u, v, p)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define function to compute the minimum distance from the nearest circular post\n",
    "def compute_distance(x, y, d, n):\n",
    "    \"\"\"Compute the minimum distance of (x, y) from the nearest circular post.\"\"\"\n",
    "    tilt = 0.4 / n\n",
    "    centers = [(0, 0), (0, 0.4), (0.4, tilt), (0.4, 0.4 + tilt)]\n",
    "    \n",
    "    distances = torch.full_like(x, float(\"inf\"))  # Initialize distances with large values\n",
    "\n",
    "    for cx, cy in centers:\n",
    "        r = d / 2\n",
    "        distance_to_post = torch.sqrt((x - cx) ** 2 + (y - cy) ** 2) - r\n",
    "        distances = torch.minimum(distances, distance_to_post)  # Take the minimum distance\n",
    "\n",
    "    return torch.maximum(distances, torch.tensor(0.0, device=device))  # Set negative distances to zero\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Extract individual elements\n",
    "x, y, d, n = input_tensor[:, 0], input_tensor[:, 1], input_tensor[:, 2], input_tensor[:, 3]\n",
    "\n",
    "# Apply conditions\n",
    "v_pred[x == 0] = 0  # If x = 0, set v_pred to zero\n",
    "\n",
    "# Compute distance\n",
    "distances = compute_distance(x, y, d, n)\n",
    "\n",
    "# If distance is zero, set u and v to zero\n",
    "mask = distances == 0\n",
    "u_pred[mask] = 0\n",
    "v_pred[mask] = 0\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab819-01e5-43e9-b954-4f54c38c145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for NO MODIFICATION\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"data/csv/DRPINN_0.3_3_1.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct columns are selected for input (same format as training)\n",
    "input_data = df.iloc[:, [0, 1, 5, 7]].values  # Selecting columns (x, y, d)\n",
    "output_data = df.iloc[:, [2, 3, 4]].values  # Selecting columns (x, y, d)\n",
    "\n",
    "# Convert to PyTorch tensor and move to GPU (if available)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1(input_tensor)\n",
    "    v_pred = model_2(input_tensor)\n",
    "    p_pred = model_3(input_tensor)\n",
    "\n",
    "# Move predictions to CPU and convert to NumPy\n",
    "u_pred = u_pred.cpu().numpy()\n",
    "v_pred = v_pred.cpu().numpy()\n",
    "p_pred = p_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131cda-ad99-4ec3-9f9b-930076f7abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = input_data[:, 0].reshape(-1)\n",
    "y_grid = input_data[:, 1].reshape(-1)\n",
    "v_actual_grid = output_data[:, 1].reshape(-1)\n",
    "v_pred_grid = v_pred.flatten()\n",
    "u_actual_grid = output_data[:, 0].reshape(-1)\n",
    "u_pred_grid = u_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9beeee-c7e0-4bc5-95f2-b8c1f08b41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x_grid,\n",
    "    'y': y_grid,\n",
    "    'u': u_pred_grid,\n",
    "    'v': v_pred_grid\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('data/csv/vW_0.55_14.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83f0c-41dc-43c0-a508-01e19fa44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pred_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ed29b-ed83-4cae-9024-8ef28858d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14fe0d-bef8-44ca-95ca-099e46f48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ece96-a3df-4962-bbac-34c537436f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Determine the common range for colorbar\n",
    "vmin = min(v_actual_grid.min(), v_pred_grid.min())\n",
    "vmax = max(v_actual_grid.max(), v_pred_grid.max())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual u-velocity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.tricontourf(x_grid, y_grid, v_actual_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Actual u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Predicted u-velocity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.tricontourf(x_grid, y_grid, v_pred_grid, levels=50, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='u-velocity')\n",
    "plt.title('Predicted u-velocity')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e3529-e012-43fb-90c0-017a1a911073",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"models/model_1_weights_3i30i1.pth\")\n",
    "torch.save(model_2.state_dict(), \"models/model_2_weights_3i30i1.pth\")\n",
    "torch.save(model_3.state_dict(), \"models/model_3_weights_3i30i1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f51cd-9c2c-4194-a401-4f701202d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual\n",
    "\n",
    "def main():\n",
    "    # Load dataset (modify file path accordingly)\n",
    "    file_path = os.path.join(os.getcwd(), \"data/merged_dataset_REU.parquet\")\n",
    "\n",
    "    # Load data using pandas and convert to NumPy array\n",
    "    df = pd.read_parquet(file_path)  # Load Parquet file\n",
    "    data = df.to_numpy(dtype=np.float32)  # Convert to NumPy array\n",
    "    \n",
    "    dataset = CustomDataset(data)\n",
    "    drt = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=10000, shuffle=True)\n",
    "    print(drt)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "\n",
    "    # Initialize models and optimizers\n",
    "    model_1, model_2, model_3, optimizer_1, optimizer_2, optimizer_3 = initialize_models()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 250\n",
    "    mse_loss = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize minimum loss to a large value\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_u_loss, total_v_loss, total_p_loss = 0, 0, 0, 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, u_loss, v_loss, p_loss = criterion(x_batch, y_batch, model_1, model_2, model_3, mse_loss)\n",
    "    \n",
    "            optimizer_1.zero_grad()\n",
    "            optimizer_2.zero_grad()\n",
    "            optimizer_3.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_1.step()\n",
    "            optimizer_2.step()\n",
    "            optimizer_3.step()\n",
    "    \n",
    "            # Accumulate loss values\n",
    "            total_loss += loss.item()\n",
    "            total_u_loss += u_loss.item()\n",
    "            total_v_loss += v_loss.item()\n",
    "            total_p_loss += p_loss.item()\n",
    "    \n",
    "        # Compute average loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_u_loss = total_u_loss / num_batches\n",
    "        avg_v_loss = total_v_loss / num_batches\n",
    "        avg_p_loss = total_p_loss / num_batches\n",
    "        \n",
    "        # Save models if the current loss is lower than the minimum loss\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(model_1.state_dict(), f'best_model_1_epoch3_{epoch}.pth')\n",
    "            torch.save(model_2.state_dict(), f'best_model_2_epoch3_{epoch}.pth')\n",
    "            torch.save(model_3.state_dict(), f'best_model_3_epoch3_{epoch}.pth')\n",
    "            print(f\"Epoch {epoch}: New minimum loss {min_loss:.7f}, models saved\")\n",
    "    \n",
    "        if epoch % 2 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Avg Loss: {avg_loss:.7f}, Avg ULoss: {avg_u_loss:.7f}, \"\n",
    "                  f\"Avg VLoss: {avg_v_loss:.7f}, Avg PLoss: {avg_p_loss:.7f}, Time: {elapsed_time:.2f} sec\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    return model_1, model_2, model_3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
